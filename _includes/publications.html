<!-- Services Section -->
    <section id="publications" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Publications</h2>
                    <h3 class="section-subheading text-muted"></h3>
                </div>
            </div>
	    <h3>2015</h3>
	    <hr>

            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/2.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Optimization as Estimation with Gaussian Processes in Bandit Settings</h4>
                            <strong>Zi Wang</strong>, Bolei Zhou, and Stefanie Jegelka</p>
                            <p><em>arXiv preprint arXiv:1510.06423 (2015)</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses a maximum a posteriori (MAP) estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. The MAP criterion can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                                

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://arxiv.org/abs/1510.06423">
                                <span class="glyphicon glyphicon-link"></span> arXiv </a> 



                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@article{wang2015est,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2015},<br>&nbsp;&nbsp;&nbsp;&nbsp;Journal={arXiv preprint arXiv:1510.06423},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Wang, Zi and Zhou, Bolei and Jegelka, Stefanie},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Optimization as Estimation with Gaussian Processes in Bandit Settings}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a> 

                    </div>
                    
                <div class="col-md-1"></div>
            </div>
           



            <h3>2014</h3>
            <hr>
            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/3.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Fast Learning with Noise in Deep Neural Nets</h4>
                            <p>Zhiyun Lu*, <strong>Zi Wang</strong>*, and Fei Sha</p>
                            <p><em>(Spotlight Presentation) NIPS Workshop: Perturbations, Optimization, and Statistics, Quebec, Canada, 2014</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Dropout has been raised as an effective and simple trick to combat overfitting
                            in deep neural nets. The idea is to randomly mask out input and internal units
                            during training. Despite its usefulness, there has been very little and scattered
                            understanding on injecting noise to deep learning architectures’ internal units. In
                            this paper, we study the effect of dropout on both input and hidden layers in deep
                            neural nets via explicit formulation of an equivalent marginalization regularizer.
                            We show that training with regularizer from marginalized noise in deep neural nets
                            doesn’t loose much performance compared to dropout, yet in significantly shorter
                            amount of training time and noticeably less sensitivity to hyperparameter tuning,
                            which are main practical concerns of dropout.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/fast_learning_with_noise_in_DNN.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>              
                        
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@article{lu_nipspos_2014,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2014},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {NIPS Workshop: Perturbations, Optimization, and Statistics},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Lu, Zhiyun and Wang, Zi and Sha, Fei},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Fast Learning with Noise in Deep Neural Nets}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a>   

                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/1.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Discriminative Non-Negative Matrix Factorization for Single-Channel Speech Separation</h4>
                            <p><strong>Zi Wang</strong> and Fei Sha</p>
                            <p><em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), Florence, Italy, 2014</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Non-negative matrix factorization (NMF) has emerged as a
                            promising approach for single-channel speech separation. In
                            this paper, we propose a new method of discriminative learn-
                            ing of NMF. In contrast to conventional approaches where the
                            basis vectors are learned independently on clean signals from
                            each speaker, our approach optimizes all basis vectors jointly
                            to reconstruct both clean signals and mixed signals well. Our
                            empirical studies validated our approach. Specifically, dis-
                            criminative NMF outperforms standard methods by a large
                            margin in improving signal-to-noise ratio for reconstructing
                            signals.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/dnmf.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                                
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/dnmf">
                                <span class="glyphicon glyphicon-home"></span> Project page </a>

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6854302">
                                <span class="glyphicon glyphicon-link"></span> IEEE </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/dnmf/wiml.pdf">
                                <span class="glyphicon glyphicon-list-alt"></span> Poster </a>   

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@inproceedings{wang_icassp_2014,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2014},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Wang, Zi and Sha, Fei},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Discriminative Non-Negative Matrix Factorization for Single-Channel Speech Separation}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a>            
                        
                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <h3>2013</h3>
            <hr>

            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/2.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Scalable Inference for Logistic-Normal Topic Models</h4>
                            <p>Jianfei Chen, Jun Zhu, <strong>Zi Wang</strong>, Xun Zheng, and Bo Zhang</p>
                            <p><em>Advances in Neural Information Processing Systems (<b>NIPS</b>), Lake Tahoe, CA, 2013</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Logistic-normal topic models can effectively discover correlation structures
                            among latent topics. However, their inference remains a challenge because of the
                            non-conjugacy between the logistic-normal prior and multinomial topic mixing
                            proportions. Existing algorithms either make restricting mean-field assumptions
                            or are not scalable to large-scale applications. This paper presents a partially collapsed
                            Gibbs sampling algorithm that approaches the provably correct distribution
                            by exploring the ideas of data augmentation. To improve time efficiency, we further
                            present a parallel implementation that can deal with large-scale applications
                            and learn the correlation structures of thousands of topics from millions of documents.
                            Extensive empirical results demonstrate the promise.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm/scalable-ctm.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                                
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm">
                                <span class="glyphicon glyphicon-home"></span> Project page </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://papers.nips.cc/paper/4981-scalable-inference-for-logistic-normal-topic-models">
                                <span class="glyphicon glyphicon-link"></span> NIPS </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm/scalablectm_nips.pdf">
                                <span class="glyphicon glyphicon-list-alt"></span> Poster </a>

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://ml-thu.net/~jianfei/static/ScaCTM.tar.gz">
                                <span class="glyphicon glyphicon-cloud-download"></span> Code </a>

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@inproceedings{chen_nips_2013,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2013},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {Advances in Neural Information Processing Systems},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Chen, Jianfei and Zhu, Jun and Wang, Zi and Zheng, Xun and Zhang, Bo},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Scalable Inference for Logistic-Normal Topic Models}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a> 

                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <hr>


        </div>
    </section>

<!-- handle abstract display -->
<script src="js/jquery.min.js"></script>  
<script src="js/bootstrap.min.js"></script>  
<script>  
$(function ()  
{ 
$('.abs').popover();  
});  
</script>  
