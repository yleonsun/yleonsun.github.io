<!-- Services Section -->
    <section id="publications" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Publications</h2>
                    <h3 class="section-subheading text-muted"></h3>
                </div>
            </div>
	    <h3>2016</h3>
	    
           <hr>

            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/6.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Focused Model-Learning and Planning for Non-Gaussian Continuous State-Action Systems</h4>
                            <strong>Zi Wang</strong>, Stefanie Jegelka, Leslie Pack Kaelbling, Tomás Lozano-Pérez</p>
                            <p><em>arXiv preprint arXiv:1607.07762</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            We introduce a joint framework for model leaning and planning in stochastic domains with continuous state and action spaces with non-Gaussian transition noise. It is efficient in large spaces with large amounts of data because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows that the expected difference between the optimal value function of the original problem and the value of the policy we compute vanishes sub-linearly in the number of actions we test, under mild assumptions. We show empirically that multi-modal transition models are necessary if the underlying dynamics is not single-mode, and our algorithm is able to complete both learning and planning within minutes for a stochastic pushing problem in simulation given more than a million data points, as a result of focused planning.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/fmlp.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://arxiv.org/abs/1607.07762">
                                <span class="glyphicon glyphicon-link"></span> arXiv </a> 


                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/fmlp_push.mp4">
                                <span class="glyphicon glyphicon-cloud-download"></span> Video </a>

                                

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content="<pre style=&quot word-wrap: break-word; white-space: pre-wrap;&quot>@article{wang2016focused,
  title={Focused Model-Learning and Planning for Non-Gaussian Continuous State-Action Systems},
  author={Wang, Zi and Jegelka, Stefanie and Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={arXiv preprint arXiv:1607.07762},
  year={2016}
}
</pre>"
                          

                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a> 

                    </div>
                    
                <div class="col-md-1"></div>
            </div>

            <hr>

            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/5.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Optimization as Estimation with Gaussian Processes in Bandit Settings</h4>
                            <strong>Zi Wang</strong>, Bolei Zhou, and Stefanie Jegelka</p>
                            <p><em>International Conference on Artificial Intelligence and Statistics (AISTATS), 2016</em></p>
                            <p><strong>Oral presentation</strong> (6% acceptance rate)</p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. '
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/wang-aistats16.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://arxiv.org/abs/1510.06423">
                                <span class="glyphicon glyphicon-link"></span> arXiv </a> 


                            <a role="button" class="btn btn-default btn-sm"
                                href="https://github.com/zi-w/GP-EST">
                                <span class="glyphicon glyphicon-cloud-download"></span> Code </a>

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gp-est">
                                <span class="glyphicon glyphicon-home"></span> Project page </a>
                                

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@inproceedings{wang2016est,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2016},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {AISTATS},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Wang, Zi and Zhou, Bolei and Jegelka, Stefanie},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Optimization as Estimation with Gaussian Processes in Bandit Settings}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a> 

                    </div>
                    
                <div class="col-md-1"></div>
            </div>

            <h3>2014</h3>
            <hr>
            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/3.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Fast Learning with Noise in Deep Neural Nets</h4>
                            <p>Zhiyun Lu*, <strong>Zi Wang</strong>*, and Fei Sha</p>
                            <p><em>(Spotlight Presentation) NIPS Workshop: Perturbations, Optimization, and Statistics, Quebec, Canada, 2014</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Dropout has been raised as an effective and simple trick to combat overfitting
                            in deep neural nets. The idea is to randomly mask out input and internal units
                            during training. Despite its usefulness, there has been very little and scattered
                            understanding on injecting noise to deep learning architectures’ internal units. In
                            this paper, we study the effect of dropout on both input and hidden layers in deep
                            neural nets via explicit formulation of an equivalent marginalization regularizer.
                            We show that training with regularizer from marginalized noise in deep neural nets
                            doesn’t loose much performance compared to dropout, yet in significantly shorter
                            amount of training time and noticeably less sensitivity to hyperparameter tuning,
                            which are main practical concerns of dropout.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/fast_learning_with_noise_in_DNN.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>              
                        
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@article{lu_nipspos_2014,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2014},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {NIPS Workshop: Perturbations, Optimization, and Statistics},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Lu, Zhiyun and Wang, Zi and Sha, Fei},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Fast Learning with Noise in Deep Neural Nets}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a>   

                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <hr>
            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/1.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Discriminative Non-Negative Matrix Factorization for Single-Channel Speech Separation</h4>
                            <p><strong>Zi Wang</strong> and Fei Sha</p>
                            <p><em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), Florence, Italy, 2014</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Non-negative matrix factorization (NMF) has emerged as a
                            promising approach for single-channel speech separation. In
                            this paper, we propose a new method of discriminative learn-
                            ing of NMF. In contrast to conventional approaches where the
                            basis vectors are learned independently on clean signals from
                            each speaker, our approach optimizes all basis vectors jointly
                            to reconstruct both clean signals and mixed signals well. Our
                            empirical studies validated our approach. Specifically, dis-
                            criminative NMF outperforms standard methods by a large
                            margin in improving signal-to-noise ratio for reconstructing
                            signals.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/pub/dnmf.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                                
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/dnmf">
                                <span class="glyphicon glyphicon-home"></span> Project page </a>

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6854302">
                                <span class="glyphicon glyphicon-link"></span> IEEE </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/dnmf/wiml.pdf">
                                <span class="glyphicon glyphicon-list-alt"></span> Poster </a>   

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@inproceedings{wang_icassp_2014,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2014},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Wang, Zi and Sha, Fei},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Discriminative Non-Negative Matrix Factorization for Single-Channel Speech Separation}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a>            
                        
                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <h3>2013</h3>
            <hr>

            <div class="row">
                <div class="col-md-1"></div>
                
                    <div class="col-md-2 text-center">
                        <img class="img-thumbnail img-responsive" src="img/pub/2.png" alt="">
                    </div>
                    <div class="col-md-8 text-left">

                            <h4>Scalable Inference for Logistic-Normal Topic Models</h4>
                            <p>Jianfei Chen, Jun Zhu, <strong>Zi Wang</strong>, Xun Zheng, and Bo Zhang</p>
                            <p><em>Advances in Neural Information Processing Systems (<b>NIPS</b>), Lake Tahoe, CA, 2013</em></p>
                            
                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '
                            Logistic-normal topic models can effectively discover correlation structures
                            among latent topics. However, their inference remains a challenge because of the
                            non-conjugacy between the logistic-normal prior and multinomial topic mixing
                            proportions. Existing algorithms either make restricting mean-field assumptions
                            or are not scalable to large-scale applications. This paper presents a partially collapsed
                            Gibbs sampling algorithm that approaches the provably correct distribution
                            by exploring the ideas of data augmentation. To improve time efficiency, we further
                            present a parallel implementation that can deal with large-scale applications
                            and learn the correlation structures of thousands of topics from millions of documents.
                            Extensive empirical results demonstrate the promise.'
                            >
                                <span class="glyphicon glyphicon-new-window"></span> Abstract
                            </a>             
                            
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm/scalable-ctm.pdf">
                                <span class="glyphicon glyphicon-file"></span> PDF </a>     

                                
                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm">
                                <span class="glyphicon glyphicon-home"></span> Project page </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://papers.nips.cc/paper/4981-scalable-inference-for-logistic-normal-topic-models">
                                <span class="glyphicon glyphicon-link"></span> NIPS </a> 

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://zi-wang.com/gctm/scalablectm_nips.pdf">
                                <span class="glyphicon glyphicon-list-alt"></span> Poster </a>

                            <a role="button" class="btn btn-default btn-sm"
                                href="http://ml-thu.net/~jianfei/static/ScaCTM.tar.gz">
                                <span class="glyphicon glyphicon-cloud-download"></span> Code </a>

                            <a class="abs btn btn-default btn-sm" class="btn btn-large btn-danger" data-toggle="popover" title="" data-placement='right'
                              data-content=
                            '<p class="bibtex">@inproceedings{chen_nips_2013,<br>&nbsp;&nbsp;&nbsp;&nbsp;Year = {2013},<br>&nbsp;&nbsp;&nbsp;&nbsp;Booktitle = {Advances in Neural Information Processing Systems},<br>&nbsp;&nbsp;&nbsp;&nbsp;Author = {Chen, Jianfei and Zhu, Jun and Wang, Zi and Zheng, Xun and Zhang, Bo},<br>&nbsp;&nbsp;&nbsp;&nbsp;Title = {Scalable Inference for Logistic-Normal Topic Models}<br>}</p>'
                             data-html="true">
                                <span class="glyphicon glyphicon-edit"></span> BibTex
                            </a> 

                    </div>
                    
                <div class="col-md-1"></div>
            </div>
            <hr>


        </div>
    </section>

<!-- handle abstract display -->
<script src="js/jquery.min.js"></script>  
<script src="js/bootstrap.min.js"></script>  
<script>  
$(function ()  
{ 
$('.abs').popover();  
});  
</script>  
